# Training configuration for pretraining and fine-tuning tiny LLM
# Optimized for a single NVIDIA RTX 3080 10GB

pretrain:
  model:
    dim: 256
    n_layers: 4
    n_heads: 4
    max_seq_len: 512
    vocab_size: 50257
    dropout: 0.1
  training:
    batch_size: 4          # Keep small to fit in VRAM
    epochs: 10
    lr: 3.0e-4
    optimizer: adamw
    grad_accum_steps: 4    # Gradient accumulation to simulate larger batches
    mixed_precision: true  # fp16 to save VRAM
    save_path: "tiny_llm_pretrained.pt"

finetune_lora:
  lora:
    r: 8
    alpha: 16
    dropout: 0.1
  training:
    batch_size: 4
    epochs: 5
    lr: 3.0e-4
    optimizer: adamw
    grad_accum_steps: 4
    mixed_precision: true
    pretrained_path: "tiny_llm_pretrained.pt"
    save_path: "tiny_llm_lora_finetuned.pt"

device:
  use_cuda: true
  cuda_device: 0